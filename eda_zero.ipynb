{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import re\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.float', '{:.3f}'.format)\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f\"./data/loan_data_2007_2014.csv\"\n",
    "input_data = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_348203/823853252.py:1: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  main_data = pd.read_csv(data_path)\n"
     ]
    }
   ],
   "source": [
    "main_data = pd.read_csv(data_path)\n",
    "data = main_data.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(include=\"object\").columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets confirm that our contunous varibles are numeric. Lets get rid of stirngs and alphanumeircs. Strip the number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.grade.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.term.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_term(val): return np.float32(re.search(r'\\d+', str(val)).group()) if re.search(r'\\d+', str(val)) else np.float32(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"term\"] = data[\"term\"].apply(transform_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data.term[0]))\n",
    "data.term.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.emp_length.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_empl_length(val):\n",
    "    return np.float64(re.search(r'\\d+', str(val)).group()) if re.search(r'\\d+', str(val)) else np.float64(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"emp_length\"] = data[\"emp_length\"].apply(transform_empl_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data[\"emp_length\"][0]))\n",
    "data.emp_length.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "earliest_cr_line: The month the borrower's earliest reported credit line was opened. Here we will count the number of months past since the earliest credit line was issued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.earliest_cr_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.earliest_cr_line.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date_by_month(val):\n",
    "    if pd.isna(val):\n",
    "        return pd.NA  # Return NaN if the input is NaN\n",
    "    else:\n",
    "        converted = pd.to_datetime(val, format=\"%b-%y\")\n",
    "        months_difference = (pd.to_datetime(\"2023-11-01\") - converted) / pd.Timedelta(4, 'W')\n",
    "        rounded_months = round(pd.to_numeric(months_difference))\n",
    "        return rounded_months\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"earliest_cr_line_date\"] = pd.to_datetime(data[\"earliest_cr_line\"], format=\"%b-%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data[\"months_since_earliest_cr_line\"] = round(pd.to_numeric((pd.to_datetime(\"2023-11-01\") - data[\"earliest_cr_line_date\"])/ pd.Timedelta(4, 'W')))\n",
    "data[\"months_since_earliest_cr_line\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have detected the negative time since the earliest credit line date. Lets explore more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"earliest_cr_line_date\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, [\"earliest_cr_line\", \"earliest_cr_line_date\", \"months_since_earliest_cr_line\"]][data[\"months_since_earliest_cr_line\"] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that many rows have future time time such as Sep-62. These variables are incorreectly converted into Sept 2062 - becuase the origin of the built-in time scale starts after 1970. We colud solve this issue by preprocessing the data or impute with maximum values of this variable. I will choose later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"months_since_earliest_cr_line\"] < 0, \"months_since_earliest_cr_line\"] = data[\"months_since_earliest_cr_line\"].max()\n",
    "data[\"months_since_earliest_cr_line\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at issued at date (issue_d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.issue_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"issue_d_date\"] = pd.to_datetime(data[\"issue_d\"], format=\"%b-%y\")\n",
    "data[\"months_since_issued_date\"] = round(pd.to_numeric((pd.to_datetime(\"2023-11-01\") - data[\"issue_d_date\"])/ pd.Timedelta(4, \"W\")))\n",
    "print(type(data[\"months_since_issued_date\"][0]))\n",
    "data[\"months_since_issued_date\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objLs = data.select_dtypes(include=\"object\").columns.values\n",
    "for col in objLs:\n",
    "    print(f\"{data[col].value_counts()}\\n==========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets explore discrete variables. I am going to use one-hot encoding for this exploration. The feature variable will increase by k-1 for each category in each variable. Variables such as:\n",
    "- grade\n",
    "- sub_grade\n",
    "- home_ownership\n",
    "- verification status \n",
    "- loan_status \n",
    "- purpose\n",
    "- addr_state\n",
    "- initial_list_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_data = []\n",
    "one_hot_variables = [\"grade\", \"home_ownership\", \"verification_status\", \"purpose\", \"addr_state\", \"initial_list_status\"]\n",
    "\n",
    "for category in one_hot_variables:\n",
    "    one_hot_data.append(pd.get_dummies(data[category], prefix=category, prefix_sep=\":\"))\n",
    "\n",
    "one_hot_data = pd.concat(one_hot_data, axis=1)\n",
    "print(f\"shape: {one_hot_data.shape}, type: {type(one_hot_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concatenate with dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.concat([data, one_hot_data], axis=1)\n",
    "print(f\"new shape:{data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets deal with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets find out the missing number and drop the varibale id it is missing more than 70% of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "droppables = []\n",
    "imputabels = []\n",
    "\n",
    "for variable in data.columns.values:\n",
    "    ratio = data[variable].isnull().sum() / data.shape[0]\n",
    "    if ratio > 0.87: droppables.append(variable)\n",
    "    if ratio > 0 and ratio < 0.87: imputabels.append(variable)\n",
    "\n",
    "print(f\"drops ({len(droppables)}): {droppables}\\nimputables ({len(imputabels)}): {imputabels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before dropping cols: {data.shape}\")\n",
    "data.drop(columns=droppables, inplace=True)\n",
    "print(f\"After dropping cols: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total revolving high credit limit / credit limit, Failry, the missing values are equal to funded_amnt. Similarly, we have only 4 missing entries in annual_inc variable. It would be reasonable to fill with mean values,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total_rev_hi_lim'].fillna(data['funded_amnt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"annual_inc\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"annual_inc\"].fillna(data[\"annual_inc\"].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picked numeric faetures \n",
    "cols_to_impute_with_zeros = [\"months_since_earliest_cr_line\", \"mths_since_last_record\", \"acc_now_delinq\", \"total_acc\", \"pub_rec\", \"open_acc\", \"inq_last_6mths\", \"delinq_2yrs\", \"emp_length\"]\n",
    "for col in cols_to_impute_with_zeros:\n",
    "    data[col].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_data = data.isnull().sum()\n",
    "null_data = null_data.loc[null_data > 0].sort_values()\n",
    "\n",
    "plt.bar(null_data.index, null_data,  color=\"blue\")\n",
    "plt.title(\"Null Values\")\n",
    "plt.xlabel(\"Variables\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(data[\"loan_status\"].value_counts().index, data[\"loan_status\"].value_counts(),  color=\"blue\")\n",
    "plt.title(\"Loan Status Bar chart\")\n",
    "plt.xlabel(\"Loan Status\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a target variable. Here, I will create a good loan vs bad loan variable (good_bad). 1- Good (non-defaulters), 0 - Bad (defaulters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaulter_label = [\"Charged Off\", \"Default\", \"Does not meet the credit policy. Status:Charged Off\", \"Late (31-120 days)\"]\n",
    "\n",
    "data[\"good_bad\"] = np.where(data[\"loan_status\"].isin(defaulter_label), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {0: 'Bad Loans', 1: 'Good Loans'}\n",
    "temp_loan_data = data[\"good_bad\"].map(label_mapping)\n",
    "\n",
    "plt.pie(temp_loan_data.value_counts(), labels=temp_loan_data.value_counts().index,autopct='%2.2f%%', startangle=90, colors=['lightgreen', 'lightcoral'] )\n",
    "plt.title(\"Target Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### independent variables \n",
    "\n",
    "How will we categorise continious variables into categorical varibales. Binining, but how do we define the range ? Fine classing.\n",
    "\n",
    "##### Fine Classing \n",
    "Fine classing, also known as fine binning, is a data preprocessing technique used in statistical analysis and machine learning to group continuous or numerical data into a larger number of smaller, more specific intervals or bins. This process is an extension of the binning or discretization process, which involves dividing a continuous variable into intervals or bins.\n",
    "\n",
    "Fine classing aims to capture more detailed patterns and variations in the data by creating smaller intervals. This can be particularly useful when working with datasets that have a wide range or exhibit non-linear relationships.\n",
    "\n",
    "The process of fine classing involves the following steps:\n",
    "\n",
    "- Determine Number of Bins: Unlike coarse binning, where a smaller number of bins are used for a broader overview, fine classing involves choosing a larger number of bins to create more detailed intervals.\n",
    "\n",
    "- Define Bin Boundaries: Establish the boundaries of each bin, ensuring that they are well-distributed across the range of the variable. The choice of bin boundaries can affect the interpretability and performance of the model.\n",
    "\n",
    "- Assign Data Points to Bins: Place each data point into its corresponding bin based on the defined boundaries. This step is crucial for subsequent analysis or modeling.\n",
    "\n",
    "Fine classing is often applied in various domains, including finance, marketing, and healthcare. In finance, for example, it might be used to create fine classes for credit score ranges to better assess the risk associated with different credit profiles. In marketing, it could be applied to customer age or income data for targeted campaign strategies. In healthcare, fine classing might be used to analyze patient data for personalized treatment plans.\n",
    "\n",
    "The choice between fine classing and other binning techniques depends on the specific characteristics of the dataset and the goals of the analysis or modeling task. Fine classing may provide more detailed insights but may also lead to overfitting or increased computational complexity in certain situations. As with any preprocessing technique, it is important to carefully consider its implications for the downstream analysis or modeling.\n",
    "\n",
    "\n",
    "Here, we are dealing with the categorical variables. we need to know the ability of each categories to predict the correct outcome. \n",
    "\n",
    "##### Weight of evidence \n",
    "\n",
    "Weight of Evidence (WoE) was developed primarily for the credit and financial industries to help build more predictive models to evaluate the risk of loan default. That is, to predict how likely the money lent to a person or institution is to be lost. Thus, Weight of Evidence is a measure of the \"strengthâ€ of a grouping technique to separate good and bad risk (default).\n",
    "\n",
    "- WoE will be 0 if the $P(Goods) / P(Bads) = 1$, that is, if the outcome is random for that group.\n",
    "- If $P(Bads) > P(Goods)$ the odds ratio will be $< 1$ and,\n",
    "- WoE will be $< 0$ if, $P(Goods) > P(Bads).$\n",
    "  \n",
    "WoE is well suited for Logistic Regression, because the Logit transformation is simply the log of the odds, i.e., \n",
    "\n",
    "\n",
    "$WoE_i$ : $ln(P(Goods)/P(Bads))$. The WoE are better far from zero. \n",
    "\n",
    "\n",
    "Suppose we have K categories, and we have computed WoE for each of the categories. \n",
    "\n",
    "##### Information value: \n",
    "\n",
    "Information Value (IV) is a measure commonly used in credit scoring and binary classification problems to assess the predictive power of an independent variable (feature) with respect to the target variable. It is particularly popular in the context of credit risk modeling.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum_{i=1}^{k} (\\%good - \\% bad) \\cdot \\ln\\left(\\frac{\\%good}{\\%bad}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "The Information Value provides a single, consolidated measure of the discriminatory power of the variable. Higher Information Values indicate stronger predictive power. Here's how Information Value is typically interpreted:\n",
    "\n",
    "- $IV < 0.02 $ : No predictive power\n",
    "- $0.02 < IV < 0.1 $ :  Weak predictive power\n",
    "- $0.1 < IV < 0.3 $ : Medium predictive power\n",
    "- $0.3 < IV < 0.5 $ :  Strong predictive power\n",
    "- $IV > 0.5 $ :  Suspisciously high\n",
    "\n",
    "\n",
    "- IV is based on the concept of entropy: It measures the reduction in entropy (uncertainty) about the target variable achieved by dividing the data into different categories based on the independent variable.\n",
    "\n",
    "- It helps in feature selection: Variables with low or zero Information Value may be candidates for removal from the model, as they contribute little to the predictive power.\n",
    "\n",
    "- Monotonicity: In practice, it's often desirable for the Information Value to exhibit a monotonic relationship with the predictive power. This means that as you move through the categories of a variable, the Information Value should generally increase or decrease.\n",
    "\n",
    "*There are more advanced methods to use, as it uses WoE which leads to overfitting*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(['good_bad'], axis=1), data[\"good_bad\"], test_size=0.2, random_state=45)\n",
    "print(f\"X tains shape: {x_train.shape},Y train shape: {y_train.shape}, test shape: {x_test.shape}, Y test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    x_train_prep = x_train.copy()\n",
    "    y_train_prep = y_train.copy()\n",
    "else:\n",
    "    x_train_prep = x_test.copy()\n",
    "    y_train_prep = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grade = pd.concat([x_train_prep['grade'], y_train_prep], axis=1)\n",
    "df_grade.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WoE of Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grade.groupby(df_grade.columns.values[0], as_index=False)[df_grade.columns.values[1]].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grade.groupby(df_grade.columns.values[0], as_index=False)[df_grade.columns.values[1]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grade = pd.concat([df_grade.groupby(df_grade.columns.values[0], as_index=False)[df_grade.columns.values[1]].count(),\n",
    "                      df_grade.groupby(df_grade.columns.values[0], as_index=False)[df_grade.columns.values[1]].mean()], axis=1)\n",
    "\n",
    "df_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grade = df_grade.iloc[:, [0,1,3]]\n",
    "df_grade.columns = [df_grade.columns.values[0], \"n_observation\", \"prop_good\"]\n",
    "df_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grade[\"prop_n_observation\"] = df_grade[\"n_observation\"]/df_grade[\"n_observation\"].sum()\n",
    "df_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number fof good borrowers and bad borrwers from each category \n",
    "df_grade[\"n_good\"] = df_grade[\"prop_good\"] * df_grade[\"n_observation\"]\n",
    "df_grade[\"n_bad\"] = (1 - df_grade[\"prop_good\"]) * df_grade[\"n_observation\"]\n",
    "\n",
    "# proportion of good and bad loans in each category \n",
    "df_grade[\"prop_n_good\"] = df_grade[\"n_good\"] / df_grade[\"n_good\"].sum()\n",
    "df_grade[\"prop_n_bad\"] = df_grade[\"n_bad\"] / df_grade[\"n_bad\"].sum()\n",
    "\n",
    "df_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grade['woe'] = np.log(df_grade[\"prop_n_good\"]/df_grade[\"prop_n_bad\"])\n",
    "df_grade = df_grade.sort_values([\"woe\"])\n",
    "df_grade.reset_index(drop=True)\n",
    "df_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grade[\"diff_prop_good\"] = df_grade[\"prop_good\"].diff().abs()\n",
    "df_grade[\"diff_woe\"] = df_grade[\"woe\"].diff().abs()\n",
    "df_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets calculate information value \n",
    "df_grade[\"IV\"] = ((df_grade[\"prop_n_good\"] - df_grade[\"prop_n_bad\"]) * df_grade[\"woe\"]).sum()\n",
    "df_grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets automate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_woe_iv(df, variable_name, target_df):\n",
    "    copy_df = pd.concat([df[variable_name], target_df], axis=1)\n",
    "    copy_df = pd.concat([copy_df.groupby(variable_name, as_index=False)[copy_df.columns.values[1]].count(),\n",
    "                         copy_df.groupby(copy_df[variable_name], as_index=False)[copy_df.columns.values[1]].mean()\n",
    "                         ], axis=1)\n",
    "    \n",
    "    copy_df = copy_df.iloc[:, [0,1,3]]\n",
    "    copy_df.columns = [copy_df.columns.values[0], \"n_observation\", \"prop_good\"]\n",
    "    copy_df[\"prop_n_observation\"] = copy_df[\"n_observation\"] / copy_df[\"n_observation\"].sum()\n",
    "    # it returns the proprtion of good loans from df\n",
    "    copy_df[\"n_good\"] = copy_df[\"prop_good\"] * copy_df[\"n_observation\"]\n",
    "    copy_df[\"n_bad\"] = (1- copy_df[\"prop_good\"]) * copy_df[\"n_observation\"]\n",
    "    \n",
    "    copy_df[\"prop_n_good\"] = copy_df[\"n_good\"] / copy_df[\"n_good\"].sum()\n",
    "    copy_df[\"prop_n_bad\"] = copy_df[\"n_bad\"] / copy_df[\"n_bad\"].sum()\n",
    "    \n",
    "    copy_df[\"woe\"] = np.log(copy_df[\"prop_n_good\"] / copy_df[\"prop_n_bad\"])\n",
    "    copy_df[\"iv\"] = ((copy_df[\"prop_n_good\"] - copy_df[\"prop_n_bad\"]) * copy_df[\"woe\"]).sum()\n",
    "    \n",
    "    copy_df = copy_df.sort_values([\"woe\"])\n",
    "    copy_df = copy_df.reset_index(drop=True)\n",
    "    return copy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_woe = compute_woe_iv(x_train_prep, \"grade\", y_train_prep)\n",
    "g_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_woe(woe, x_tick_rotation=0):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(woe.iloc[:, 0].apply(str), woe['woe'], marker='o', linestyle='--', color='b', label=f'{woe.columns.values[0]} WoE Line chart')\n",
    "    plt.xlabel(woe.columns.values[0])\n",
    "    plt.xticks(rotation = x_tick_rotation)\n",
    "    plt.ylabel(\"Weight of evidence\")\n",
    "    plt.title(f\"Weight of evidence by {woe.columns.values[0]}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(g_woe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grades have monotonically increase from G(worse) to A(best). The greater the grade better the weight of evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_ref_categories = []\n",
    "list_of_dummy_variables = []\n",
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([item for item in x_train_prep.columns.values if item.startswith('grade:')])\n",
    "    list_of_ref_categories.append(list_of_dummy_variables[-1])\n",
    "    print(f\"{list_of_dummy_variables}\\n{list_of_ref_categories}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_ownership_woe = compute_woe_iv(x_train_prep, \"home_ownership\", y_train_prep)\n",
    "home_ownership_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(home_ownership_woe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookin at the Chart, The categories other, and None are associated with highest probability of default. WoE tells us that these categories have very few loans (< 0.1% each), and only one observation with ANY home ownership status. It does not make sense to have dummy varibles, neither to loose data. Hence, we combine such under represented category of similar nature. we will cobmined those one with riskiest variable Rent. Final dummy varibles [Rent, OWN,Mortgage ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep[\"home_ownership:RENT_OTHER_NONE_ANY\"] = sum([x_train_prep[\"home_ownership:RENT\"], x_train_prep[\"home_ownership:NONE\"], x_train_prep[\"home_ownership:OTHER\"], x_train_prep[\"home_ownership:ANY\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([\"home_ownership:RENT_OTHER_NONE_ANY\", \"home_ownership:OWN\", \"home_ownership:MORTGAGE\"])\n",
    "    list_of_ref_categories.append(\"home_ownership:RENT_OTHER_NONE_ANY\")\n",
    "    print(f\"{list_of_dummy_variables}\\n{list_of_ref_categories}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr_state_woe = compute_woe_iv(x_train_prep, \"addr_state\", y_train_prep)\n",
    "addr_state_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(addr_state_woe, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the graph, The NE, IA, and ID have very less observations, which has led them to have extreme WoE values. Lets cut them out and observe rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not see the north dakota here. We want to add if that is true\n",
    "if \"addr_state:ND\" not in x_train_prep.columns.values: x_train_prep['addr_state:ND'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(addr_state_woe.iloc[3:-1,:], 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this chart, we can categorise multiple states into one such as NE, IA, NV, AL, ID, ND (as we have no information about) FL, HI into one as they have similar woe. Similarly, WV, WY, DC, and ME in another categoruy, and CO, VT, MS, NH, AK, MT in another and similar for other states. we can notice that NY, CA, TX havehigher number of observation, hence we will leave them as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(addr_state_woe.iloc[7:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep[\"addr_state:NE_IA_NV_AL_ID_ND_FL_HI\"] = sum([x_train_prep['addr_state:ND'], x_train_prep['addr_state:NE'],\n",
    "                                                         x_train_prep['addr_state:IA'], x_train_prep['addr_state:NV'],\n",
    "                                                         x_train_prep['addr_state:FL'], x_train_prep['addr_state:HI'],\n",
    "                                                         x_train_prep['addr_state:AL'], x_train_prep['addr_state:ID']])\n",
    "\n",
    "\n",
    "x_train_prep[\"addr_state:LA_MO_DE_MD\"] = sum([x_train_prep['addr_state:LA'], x_train_prep['addr_state:MO'], \n",
    "                                              x_train_prep['addr_state:DE'], x_train_prep['addr_state:MD']])\n",
    "\n",
    "x_train_prep['addr_state:NC_NM'] = sum([x_train_prep['addr_state:NC'], x_train_prep['addr_state:NM']])\n",
    "\n",
    "\n",
    "x_train_prep['addr_state:NJ_AZ_VA_OK_TN'] = sum([x_train_prep['addr_state:NJ'], x_train_prep['addr_state:AZ'],\n",
    "                                              x_train_prep['addr_state:VA'], x_train_prep['addr_state:OK'],\n",
    "                                              x_train_prep['addr_state:TN']])\n",
    "\n",
    "x_train_prep['addr_state:AR_PR_MI'] = sum([x_train_prep['addr_state:AR'], x_train_prep['addr_state:PA'],\n",
    "                                              x_train_prep['addr_state:MI']])\n",
    "\n",
    "\n",
    "x_train_prep['addr_state:UT_MA_RI_OH'] = sum([x_train_prep['addr_state:UT'], x_train_prep['addr_state:MA'],\n",
    "                                              x_train_prep['addr_state:RI'], x_train_prep['addr_state:OH']])\n",
    "\n",
    "\n",
    "x_train_prep['addr_state:KY_MN_GA_SD'] = sum([x_train_prep['addr_state:KY'], x_train_prep['addr_state:MN'],\n",
    "                                              x_train_prep['addr_state:GA'], x_train_prep['addr_state:SD']])\n",
    "\n",
    "\n",
    "\n",
    "x_train_prep['addr_state:IN_OR_WA_WI'] = sum([x_train_prep['addr_state:IN'], x_train_prep['addr_state:WA'],\n",
    "                                              x_train_prep['addr_state:OR'], x_train_prep['addr_state:WI'] ])\n",
    "\n",
    "x_train_prep['addr_state:IL_SC_CT_KS_CO'] = sum([x_train_prep['addr_state:IL'], x_train_prep['addr_state:SC'],\n",
    "                                                 x_train_prep['addr_state:CT'], x_train_prep['addr_state:KS'],\n",
    "                                                 x_train_prep['addr_state:CO']])\n",
    "\n",
    "\n",
    "\n",
    "x_train_prep['addr_state:VT_MS_NH_AK_MT'] = sum([x_train_prep['addr_state:VT'], x_train_prep['addr_state:MS'],\n",
    "                                                    x_train_prep['addr_state:NH'], x_train_prep['addr_state:AK'],\n",
    "                                                    x_train_prep['addr_state:MT']])\n",
    "\n",
    "x_train_prep['addr_state:WV_WY_DC_ME'] = sum([x_train_prep['addr_state:WV'], x_train_prep['addr_state:WY'],\n",
    "                                              x_train_prep['addr_state:DC'], x_train_prep['addr_state:ME'] ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend(\n",
    "        [   'addr_state:NY',\n",
    "            'addr_state:CA',\n",
    "            'addr_state:TX',\n",
    "            'addr_state:NE_IA_NV_AL_ID_ND_FL_HI',\n",
    "            'addr_state:LA_MO_DE_MD',\n",
    "            'addr_state:NC_NM',\n",
    "            'addr_state:NJ_AZ_VA_OK_TN',\n",
    "            'addr_state:AR_PR_MI',\n",
    "            'addr_state:UT_MA_RI_OH',\n",
    "            'addr_state:KY_MN_GA_SD',\n",
    "            'addr_state:IN_OR_WA_WI',\n",
    "            'addr_state:IL_SC_CT_KS_CO',\n",
    "            'addr_state:VT_MS_NH_AK_MT',\n",
    "            'addr_state:WV_WY_DC_ME'\n",
    "        ]\n",
    "    )\n",
    "    list_of_ref_categories.append('addr_state:NE_IA_NV_AL_ID_ND_FL_HI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_status_woe = compute_woe_iv(x_train_prep, \"verification_status\", y_train_prep)\n",
    "verification_status_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(verification_status_woe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([item  for item in x_train_prep.columns.values if item.startswith(\"verification_status:\")])\n",
    "    list_of_ref_categories.append(list_of_dummy_variables[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose_woe = compute_woe_iv(x_train_prep, \"purpose\", y_train_prep)\n",
    "purpose_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(purpose_woe, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep['purpose:educ_small_biz_wedd_renno_enerby_moving_other_house'] = sum([x_train_prep['purpose:educational'], x_train_prep['purpose:small_business'],\n",
    "                                                                 x_train_prep['purpose:wedding'], x_train_prep['purpose:renewable_energy'],\n",
    "                                                                 x_train_prep['purpose:moving'], x_train_prep['purpose:house'],\n",
    "                                                                  x_train_prep['purpose:other']])\n",
    "x_train_prep[\"purpose:medical_vacation_wedding\"] = sum([x_train_prep[\"purpose:medical\"], x_train_prep[\"purpose:vacation\"], x_train_prep[\"purpose:wedding\"]])\n",
    "\n",
    "x_train_prep[\"purpose:major_purchase_home_imp_car\"]  = sum([x_train_prep[\"purpose:major_purchase\"], x_train_prep[\"purpose:home_improvement\"],\n",
    "                                                            x_train_prep[\"purpose:car\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([\"purpose:educ_small_biz_wedd_renno_enerby_moving_other_house\",\"purpose:medical_vacation_wedding\", \n",
    "                                    \"purpose:major_purchase_home_imp_car\", \"purpose:credit_card\", \"purpose:debt_consolidation\"])\n",
    "    list_of_ref_categories.append(\"purpose:educ_small_biz_wedd_renno_enerby_moving_other_house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_list_status_we = compute_woe_iv(x_train_prep, \"initial_list_status\", y_train_prep)\n",
    "initial_list_status_we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(initial_list_status_we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([\"initial_list_status:f\", \"initial_list_status:w\"])\n",
    "    list_of_ref_categories.append(\"initial_list_status:f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do for continous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_woe_iv_continous(df, variable_name, target_df):\n",
    "    copy_df = pd.concat([df[variable_name], target_df], axis=1)\n",
    "    copy_df = pd.concat([copy_df.groupby(variable_name, as_index=False)[copy_df.columns.values[1]].count(),\n",
    "                         copy_df.groupby(copy_df[variable_name], as_index=False)[copy_df.columns.values[1]].mean()\n",
    "                         ], axis=1)\n",
    "    \n",
    "    copy_df = copy_df.iloc[:, [0,1,3]]\n",
    "    copy_df.columns = [copy_df.columns.values[0], \"n_observation\", \"prop_good\"]\n",
    "    copy_df[\"prop_n_observation\"] = copy_df[\"n_observation\"] / copy_df[\"n_observation\"].sum()\n",
    "    # it returns the proprtion of good loans from df\n",
    "    copy_df[\"n_good\"] = copy_df[\"prop_good\"] * copy_df[\"n_observation\"]\n",
    "    copy_df[\"n_bad\"] = (1- copy_df[\"prop_good\"]) * copy_df[\"n_observation\"]\n",
    "    \n",
    "    copy_df[\"prop_n_good\"] = copy_df[\"n_good\"] / copy_df[\"n_good\"].sum()\n",
    "    copy_df[\"prop_n_bad\"] = copy_df[\"n_bad\"] / copy_df[\"n_bad\"].sum()\n",
    "    \n",
    "    copy_df[\"woe\"] = np.log(copy_df[\"prop_n_good\"] / copy_df[\"prop_n_bad\"])\n",
    "    copy_df[\"iv\"] = ((copy_df[\"prop_n_good\"] - copy_df[\"prop_n_bad\"]) * copy_df[\"woe\"]).sum()\n",
    "    \n",
    "    # copy_df = copy_df.sort_values([\"woe\"])\n",
    "    # copy_df = copy_df.reset_index(drop=True)\n",
    "    return copy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_woe = compute_woe_iv_continous(x_train_prep, \"term\", y_train_prep)\n",
    "term_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(term_woe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep[\"term:36\"] = np.where((x_train_prep[\"term\"] == 36), 1, 0)\n",
    "x_train_prep[\"term:60\"] = np.where((x_train_prep[\"term\"] == 60), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([\"term:36\", \"term:60\"])\n",
    "    list_of_ref_categories.append(\"term:60\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_length_woe = compute_woe_iv_continous(x_train_prep, \"emp_length\", y_train)\n",
    "emp_length_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(emp_length_woe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep[\"emp_length:0\"] = np.where((x_train_prep[\"emp_length\"] == 0), 1, 0)\n",
    "x_train_prep[\"emp_length:1\"] = np.where((x_train_prep[\"emp_length\"] == 1), 1, 0)\n",
    "x_train_prep[\"emp_length:2_4\"] = np.where((x_train_prep[\"emp_length\"].isin(range(2,5))), 1, 0)\n",
    "x_train_prep[\"emp_length:5_6\"] = np.where((x_train_prep[\"emp_length\"].isin(range(5,7))), 1, 0)\n",
    "x_train_prep[\"emp_length:7_9\"] = np.where((x_train_prep[\"emp_length\"].isin(range(7, 10))), 1, 0)\n",
    "x_train_prep[\"emp_length:10\"] = np.where((x_train_prep[\"emp_length\"] == 10), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([\"emp_length:0\", \"emp_length:1\", \"emp_length:2_4\", \"emp_length:5_6\", \"emp_length:7_9\", \"emp_length:10\"])\n",
    "    list_of_ref_categories.append(\"emp_length:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep[\"months_since_issued_date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do fine and coarse classing \n",
    "x_train_prep[\"months_since_issued_date_factor\"] = pd.cut(x_train_prep[\"months_since_issued_date\"], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_months_since_issued_date_factor_woe = compute_woe_iv_continous(x_train_prep, \"months_since_issued_date_factor\", y_train_prep)\n",
    "cat_months_since_issued_date_factor_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(cat_months_since_issued_date_factor_woe, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep[\"months_since_issued_date:<117\"] = np.where(x_train_prep[\"months_since_issued_date\"].isin(range(117)), 1,0)\n",
    "x_train_prep[\"months_since_issued_date:117_119\"] = np.where(x_train_prep[\"months_since_issued_date\"].isin(range(117,119)), 1,0)\n",
    "x_train_prep[\"months_since_issued_date:119_123\"] = np.where(x_train_prep[\"months_since_issued_date\"].isin(range(119, 123)), 1,0)\n",
    "x_train_prep[\"months_since_issued_date:123_131\"] = np.where(x_train_prep[\"months_since_issued_date\"].isin(range(123, 131)), 1,0)\n",
    "x_train_prep[\"months_since_issued_date:131_149\"] = np.where(x_train_prep[\"months_since_issued_date\"].isin(range(131, 149)), 1,0)\n",
    "x_train_prep[\"months_since_issued_date:149_172\"] = np.where(x_train_prep[\"months_since_issued_date\"].isin(range(149, 172)), 1,0)\n",
    "x_train_prep[\"months_since_issued_date:>172\"] = np.where(x_train_prep[\"months_since_issued_date\"].isin(range(172, int(x_train_prep[\"months_since_issued_date\"].max()))), 1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([\"months_since_issued_date:<117\", \"months_since_issued_date:117_119\", \"months_since_issued_date:119_123\", \n",
    "                                \"months_since_issued_date:123_131\", \"months_since_issued_date:131_149\", \"months_since_issued_date:149_172\", \"months_since_issued_date:>172\"])\n",
    "    list_of_ref_categories.append(\"months_since_issued_date:>172\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep[\"int_rate_factor\"] = pd.cut(x_train_prep[\"int_rate\"], 50)\n",
    "\n",
    "int_rate_factor_woe = compute_woe_iv_continous(x_train_prep, \"int_rate_factor\", y_train_prep)\n",
    "int_rate_factor_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(int_rate_factor_woe, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep[\"int_rate:<9.54\"] = np.where((x_train_prep[\"int_rate\"]<= 9.548), 1, 0)\n",
    "x_train_prep[\"int_rate:9.54_12.025\"] = np.where((x_train_prep[\"int_rate\"]> 9.548) & (x_train_prep[\"int_rate\"]<=12.025), 1, 0)\n",
    "x_train_prep[\"int_rate:12.025_15.74\"] = np.where((x_train_prep[\"int_rate\"]> 12.025) & (x_train_prep[\"int_rate\"]<=15.74), 1, 0)\n",
    "x_train_prep[\"int_rate:15.74_20.281\"] = np.where((x_train_prep[\"int_rate\"]> 15.74) & (x_train_prep[\"int_rate\"]<=20.281), 1, 0)\n",
    "x_train_prep[\"int_rate:>20.281\"] = np.where((x_train_prep[\"int_rate\"] > 20.281), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([\"int_rate:<9.54\", \"int_rate:9.54_12.025\", \"int_rate:12.025_15.74\", \n",
    "                                \"int_rate:15.74_20.281\", \"int_rate:>20.281\"])\n",
    "    list_of_ref_categories.append(\"int_rate:>20.281\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep[\"funded_amnt_factor\"] = pd.cut(x_train_prep[\"funded_amnt\"], 50)\n",
    "funded_amount_woe = compute_woe_iv_continous(x_train_prep, \"funded_amnt_factor\", y_train_prep)\n",
    "funded_amount_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(funded_amount_woe, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart shows very different WoE on each category. It appears like trens, and almost horizontal. We can not find the pattern between indepndent and dependent variable. Hence, we will not use this varaible as predicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep[\"annual_inc_factor\"] = pd.cut(x_train_prep[\"annual_inc\"], 105)\n",
    "annual_inc_woe = compute_woe_iv_continous(x_train_prep, \"annual_inc_factor\", y_train_prep)\n",
    "annual_inc_woe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority of annual income samples are from < 144000. Here, we will split the data < 144000 and > 144000, and further categorise by usng WoE method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_income_df = x_train_prep.loc[x_train_prep[\"annual_inc\"] <= 144000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_income_df[\"annual_inc_factor\"] = pd.cut(annual_income_df[\"annual_inc\"], 50)\n",
    "annual_inc_temp_woe = compute_woe_iv_continous(annual_income_df, \"annual_inc_factor\", y_train_prep[annual_income_df.index])\n",
    "annual_inc_temp_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(annual_inc_temp_woe, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep[\"annual_inc:<20K\"] = np.where((x_train_prep[\"annual_inc\"] <= 20000), 1, 0)\n",
    "x_train_prep[\"annual_inc:<20K_30K\"] = np.where((x_train_prep[\"annual_inc\"] > 20000) & (x_train_prep[\"annual_inc\"] >= 30000) , 1, 0)\n",
    "x_train_prep[\"annual_inc:<30K_40K\"] = np.where((x_train_prep[\"annual_inc\"] > 30000) & (x_train_prep[\"annual_inc\"] <= 40000) , 1, 0)\n",
    "x_train_prep[\"annual_inc:<40K_50K\"] = np.where((x_train_prep[\"annual_inc\"] > 40000) & (x_train_prep[\"annual_inc\"] <= 50000) , 1, 0)\n",
    "x_train_prep[\"annual_inc:<50K_60K\"] = np.where((x_train_prep[\"annual_inc\"] > 50000) & (x_train_prep[\"annual_inc\"] <= 60000) , 1, 0)\n",
    "x_train_prep[\"annual_inc:<60K_70K\"] = np.where((x_train_prep[\"annual_inc\"] > 60000) & (x_train_prep[\"annual_inc\"] <= 70000) , 1, 0)\n",
    "x_train_prep[\"annual_inc:<70K_80K\"] = np.where((x_train_prep[\"annual_inc\"] > 70000) & (x_train_prep[\"annual_inc\"] <= 80000) , 1, 0)\n",
    "x_train_prep[\"annual_inc:<80K_90K\"] = np.where((x_train_prep[\"annual_inc\"] > 80000) & (x_train_prep[\"annual_inc\"] <= 90000) , 1, 0)\n",
    "x_train_prep[\"annual_inc:<90K_100K\"] = np.where((x_train_prep[\"annual_inc\"] > 90000) & (x_train_prep[\"annual_inc\"] <= 100000) , 1, 0)\n",
    "x_train_prep[\"annual_inc:<100K_120K\"] = np.where((x_train_prep[\"annual_inc\"] > 100000) & (x_train_prep[\"annual_inc\"] <= 120000) , 1, 0)\n",
    "x_train_prep[\"annual_inc:<120K_140K\"] = np.where((x_train_prep[\"annual_inc\"] > 120000) & (x_train_prep[\"annual_inc\"] <= 140000) , 1, 0)\n",
    "x_train_prep[\"annual_inc:>140K\"] = np.where((x_train_prep[\"annual_inc\"] > 140000), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([\"annual_inc:<20K\", \"annual_inc:<20K_30K\", \"annual_inc:<30K_40K\", \"annual_inc:<40K_50K\", \"annual_inc:<50K_60K\", \"annual_inc:<60K_70K\",\n",
    "                                \"annual_inc:<70K_80K\", \"annual_inc:<80K_90K\", \"annual_inc:<90K_100K\", \"annual_inc:<100K_120K\", \"annual_inc:<120K_140K\", \"annual_inc:>140K\"])\n",
    "    list_of_ref_categories.append(\"annual_inc:<20K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mths_since_last_delinq_df = x_train_prep[pd.notnull(x_train_prep[\"mths_since_last_delinq\"])]\n",
    "mths_since_last_delinq_df['mths_since_last_delinq_factor'] = pd.cut(mths_since_last_delinq_df[\"mths_since_last_delinq\"], 50)\n",
    "mths_since_last_delinq_woe = compute_woe_iv_continous(mths_since_last_delinq_df, \"mths_since_last_delinq_factor\", y_train_prep[mths_since_last_delinq_df.index])\n",
    "mths_since_last_delinq_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(mths_since_last_delinq_woe, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep[\"mths_since_last_delinq:0_3\"] = np.where((x_train_prep[\"mths_since_last_delinq\"] >= 0 & (x_train_prep[\"mths_since_last_delinq\"] < 4)), 1, 0)\n",
    "x_train_prep[\"mths_since_last_delinq:4_37\"] = np.where((x_train_prep[\"mths_since_last_delinq\"] >= 4 & (x_train_prep[\"mths_since_last_delinq\"] < 37)), 1, 0)\n",
    "x_train_prep[\"mths_since_last_delinq:37_56\"] = np.where((x_train_prep[\"mths_since_last_delinq\"] >= 37 & (x_train_prep[\"mths_since_last_delinq\"] < 56)), 1, 0)\n",
    "x_train_prep[\"mths_since_last_delinq:>56\"] = np.where((x_train_prep[\"mths_since_last_delinq\"] >= 56), 1, 0)\n",
    "x_train_prep[\"mths_since_last_delinq:missing\"] = np.where((x_train_prep[\"mths_since_last_delinq\"].isnull()), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dummy_variables.extend([\"mths_since_last_delinq:0_3\", \"mths_since_last_delinq:4_37\", \"mths_since_last_delinq:37_56\", \"mths_since_last_delinq:>56\", \"mths_since_last_delinq:missing\"])\n",
    "list_of_ref_categories.append(\"mths_since_last_delinq:0_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep[\"months_since_earliest_cr_line_factor\"] = pd.cut(x_train_prep[\"months_since_earliest_cr_line\"], 50)\n",
    "months_since_earliest_cr_line_factor_woe = compute_woe_iv_continous(x_train_prep, \"months_since_earliest_cr_line_factor\", y_train_prep)\n",
    "months_since_earliest_cr_line_factor_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(months_since_earliest_cr_line_factor_woe, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(months_since_earliest_cr_line_factor_woe.iloc[7: , : ], 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep['months_since_earliest_cr_line:<171'] = np.where(x_train_prep['months_since_earliest_cr_line'].isin(range(171)), 1, 0)\n",
    "x_train_prep['months_since_earliest_cr_line:171_228'] = np.where(x_train_prep['months_since_earliest_cr_line'].isin(range(171, 228)), 1, 0)\n",
    "x_train_prep['months_since_earliest_cr_line:228_257'] = np.where(x_train_prep['months_since_earliest_cr_line'].isin(range(228, 257)), 1, 0)\n",
    "x_train_prep['months_since_earliest_cr_line:257_371'] = np.where(x_train_prep['months_since_earliest_cr_line'].isin(range(257, 371)), 1, 0)\n",
    "x_train_prep['months_since_earliest_cr_line:371_471'] = np.where(x_train_prep['months_since_earliest_cr_line'].isin(range(371, 471)), 1, 0)\n",
    "x_train_prep['months_since_earliest_cr_line:>486'] = np.where(x_train_prep['months_since_earliest_cr_line'].isin(range(486, int(x_train_prep['months_since_earliest_cr_line'].max()))), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dummy_variables.extend([\"months_since_earliest_cr_line:<171\", \"months_since_earliest_cr_line:171_228\", \"months_since_earliest_cr_line:228_257\",\n",
    "                                \"months_since_earliest_cr_line:257_371\", \"months_since_earliest_cr_line:371_471\", \"months_since_earliest_cr_line:>486\"])\n",
    "list_of_ref_categories.append(\"months_since_earliest_cr_line:<171\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep[\"installment_factor\"] = pd.cut(x_train_prep['installment'], 50)\n",
    "installment_woe = compute_woe_iv_continous(x_train_prep, \"installment_factor\", y_train_prep)\n",
    "installment_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(installment_woe, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature does not contribute much to dependent variable. Hence, we omit this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delinq_2years_woe = compute_woe_iv_continous(x_train_prep, \"delinq_2yrs\", y_train_prep)\n",
    "delinq_2years_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(delinq_2years_woe, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep['delinq_2yrs:0'] = np.where((x_train_prep['delinq_2yrs'] == 0), 1, 0)\n",
    "x_train_prep['delinq_2yrs:1_3'] = np.where((x_train_prep['delinq_2yrs'] >= 1) & (x_train_prep['delinq_2yrs'] < 4), 1, 0)\n",
    "x_train_prep['delinq_2yrs:4_7'] = np.where((x_train_prep['delinq_2yrs'] >= 4) & (x_train_prep['delinq_2yrs'] < 7), 1, 0)\n",
    "x_train_prep['delinq_2yrs:>7'] = np.where((x_train_prep['delinq_2yrs'] >= 7), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dummy_variables.extend([\"delinq_2yrs:0\", \"delinq_2yrs:1_3\", \"delinq_2yrs:4_7\", \"delinq_2yrs:>7\"])\n",
    "list_of_ref_categories.append(\"delinq_2yrs:>7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inq_last_6mths_woe = compute_woe_iv_continous(x_train_prep, \"inq_last_6mths\", y_train_prep)\n",
    "inq_last_6mths_woe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(inq_last_6mths_woe, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep['inq_last_6mths:0'] = np.where((x_train_prep['inq_last_6mths'] == 0), 1, 0)\n",
    "x_train_prep['inq_last_6mths:1_2'] = np.where((x_train_prep['inq_last_6mths'] >= 1) & (x_train_prep['inq_last_6mths'] <= 2), 1, 0)\n",
    "x_train_prep['inq_last_6mths:3_6'] = np.where((x_train_prep['inq_last_6mths'] >= 3) & (x_train_prep['inq_last_6mths'] <= 6), 1, 0)\n",
    "x_train_prep['inq_last_6mths:>6'] = np.where((x_train_prep['inq_last_6mths'] > 6), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([\"inq_last_6mths:0\", \"inq_last_6mths:1_2\", \"inq_last_6mths:3_6\", \"inq_last_6mths:>6\"])\n",
    "    list_of_ref_categories.append(\"inq_last_6mths:>6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_acc_woe = compute_woe_iv_continous(x_train_prep, \"open_acc\", y_train_prep)\n",
    "open_acc_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(open_acc_woe, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(open_acc_woe.loc[:40,:], 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep['open_acc:<4'] = np.where((x_train_prep['open_acc'] < 4), 1, 0)\n",
    "x_train_prep['open_acc:4_12'] = np.where((x_train_prep['open_acc'] >= 4) & (x_train_prep['open_acc'] <= 12), 1, 0)\n",
    "x_train_prep['open_acc:13_17'] = np.where((x_train_prep['open_acc'] >= 13) & (x_train_prep['open_acc'] <= 17), 1, 0)\n",
    "x_train_prep['open_acc:18_22'] = np.where((x_train_prep['open_acc'] >= 18) & (x_train_prep['open_acc'] <= 22), 1, 0)\n",
    "x_train_prep['open_acc:23_25'] = np.where((x_train_prep['open_acc'] >= 23) & (x_train_prep['open_acc'] <= 25), 1, 0)\n",
    "x_train_prep['open_acc:26_30'] = np.where((x_train_prep['open_acc'] >= 26) & (x_train_prep['open_acc'] <= 30), 1, 0)\n",
    "x_train_prep['open_acc:>=31'] = np.where((x_train_prep['open_acc'] >= 31), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([\"open_acc:<4\", \"open_acc:4_12\", \"open_acc:13_17\", \"open_acc:18_22\", \"open_acc:23_25\", \"open_acc:26_30\", \"open_acc:>=31\" ])\n",
    "    list_of_ref_categories.append(\"open_acc:<4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_rec_woe = compute_woe_iv_continous(x_train_prep, \"pub_rec\", y_train_prep)\n",
    "pub_rec_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(pub_rec_woe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep['pub_rec:0_2'] = np.where((x_train_prep['pub_rec'] >= 0) & (x_train_prep['pub_rec'] <= 2), 1, 0)\n",
    "x_train_prep['pub_rec:3_4'] = np.where((x_train_prep['pub_rec'] >= 3) & (x_train_prep['pub_rec'] <= 4), 1, 0)\n",
    "x_train_prep['pub_rec:>=5'] = np.where((x_train_prep['pub_rec'] >= 5), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([\"pub_rec:0_2\", \"pub_rec:3_4\", \"pub_rec:>=5\"])\n",
    "    list_of_ref_categories.append(\"pub_rec:0_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep['total_acc_factor'] = pd.cut(x_train_prep['total_acc'], 50)\n",
    "total_acc_woe = compute_woe_iv_continous(x_train_prep, \"total_acc_factor\", y_train_prep)\n",
    "total_acc_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(total_acc_woe, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep['total_acc:<=28'] = np.where((x_train_prep['total_acc'] < 28), 1, 0)\n",
    "x_train_prep['total_acc:28_50'] = np.where((x_train_prep['total_acc'] >= 28) & (x_train_prep['total_acc'] < 50), 1, 0)\n",
    "x_train_prep['total_acc:>=50'] = np.where((x_train_prep['total_acc'] >= 50), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([\"total_acc:<=28\", \"total_acc:28_50\", \"total_acc:>=50\"])\n",
    "    list_of_ref_categories.append(\"total_acc:<=28\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_now_delinq_woe = compute_woe_iv_continous(x_train_prep, \"acc_now_delinq\", y_train_prep)\n",
    "acc_now_delinq_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(acc_now_delinq_woe, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep['acc_now_delinq:0'] = np.where((x_train_prep['acc_now_delinq'] == 0), 1, 0)\n",
    "x_train_prep['acc_now_delinq:>=1'] = np.where((x_train_prep['acc_now_delinq'] >= 1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([\"acc_now_delinq:0\", \"acc_now_delinq:>=1\"])\n",
    "    list_of_ref_categories.append(\"acc_now_delinq:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep[\"dti_factor\"] = pd.cut(x_train_prep[\"dti\"], 50)\n",
    "dti_woe = compute_woe_iv_continous(x_train_prep, \"dti_factor\", y_train_prep)\n",
    "dti_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(dti_woe, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep['dti:<=1.6'] = np.where((x_train_prep['dti'] < 1.6), 1, 0)\n",
    "x_train_prep['dti:1.6_6.39'] = np.where((x_train_prep['dti'] >= 1.4) & (x_train_prep['dti'] <= 3.5), 1, 0)\n",
    "x_train_prep['dti:6.39_10.39'] = np.where((x_train_prep['dti'] >= 6.39) & (x_train_prep['dti'] < 10.39), 1, 0)\n",
    "x_train_prep['dti:12.97_16.79'] = np.where((x_train_prep['dti'] >= 12.97) & (x_train_prep['dti'] <= 16.79), 1, 0)\n",
    "x_train_prep['dti:17.59_19.95'] = np.where((x_train_prep['dti'] >= 17.59) & (x_train_prep['dti'] <= 19.95), 1, 0)\n",
    "x_train_prep['dti:20.75_23.99'] = np.where((x_train_prep['dti'] >= 20.75) & (x_train_prep['dti'] <= 23.99), 1, 0)\n",
    "x_train_prep['dti:24.9_31.99'] = np.where((x_train_prep['dti'] >= 24.9) & (x_train_prep['dti'] <= 31.99), 1, 0)\n",
    "x_train_prep['dti:>32'] = np.where((x_train_prep['dti'] > 32), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([\"dti:<=1.6\", \"dti:1.6_6.39\", \"dti:6.39_10.39\", \"dti:12.97_16.79\", \"dti:17.59_19.95\", \"dti:20.75_23.99\", \"dti:24.9_31.99\", \"dti:>32\"])\n",
    "    list_of_ref_categories.append(\"dti:>32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep[\"mths_since_last_record_factor\"] = pd.cut(x_train_prep[\"mths_since_last_record\"], 50)\n",
    "mths_since_last_record_woe = compute_woe_iv_continous(x_train_prep, \"mths_since_last_record_factor\", y_train_prep)\n",
    "mths_since_last_record_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(mths_since_last_record_woe, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep['mths_since_last_record:0_2'] = np.where((x_train_prep['mths_since_last_record'] < 2.5), 1, 0)\n",
    "x_train_prep['mths_since_last_record:>2'] = np.where((x_train_prep['mths_since_last_record'] >= 2.5), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([\"mths_since_last_record:0_2\", \"mths_since_last_record:>2\"])\n",
    "    list_of_ref_categories.append(\"mths_since_last_record:0_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prep[\"total_rev_hi_lim_factor\"] = pd.cut(x_train_prep[\"total_rev_hi_lim\"], 2000)\n",
    "total_rev_woe = compute_woe_iv_continous(x_train_prep, \"total_rev_hi_lim_factor\", y_train_prep)\n",
    "total_rev_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(total_rev_woe.iloc[: 50, : ],90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '<=5K', '5K-10K', '10K-20K', '20K-30K', '30K-40K', '40K-55K', '55K-95K', '>95K'\n",
    "x_train_prep['total_rev_hi_lim:<=5K'] = np.where((x_train_prep['total_rev_hi_lim'] <= 5000), 1, 0)\n",
    "x_train_prep['total_rev_hi_lim:5K_10K'] = np.where((x_train_prep['total_rev_hi_lim'] > 5000) & (x_train_prep['total_rev_hi_lim'] <= 10000), 1, 0)\n",
    "x_train_prep['total_rev_hi_lim:10K_20K'] = np.where((x_train_prep['total_rev_hi_lim'] > 10000) & (x_train_prep['total_rev_hi_lim'] <= 20000), 1, 0)\n",
    "x_train_prep['total_rev_hi_lim:20K_30K'] = np.where((x_train_prep['total_rev_hi_lim'] > 20000) & (x_train_prep['total_rev_hi_lim'] <= 30000), 1, 0)\n",
    "x_train_prep['total_rev_hi_lim:30K_40K'] = np.where((x_train_prep['total_rev_hi_lim'] > 30000) & (x_train_prep['total_rev_hi_lim'] <= 40000), 1, 0)\n",
    "x_train_prep['total_rev_hi_lim:40K_55K'] = np.where((x_train_prep['total_rev_hi_lim'] > 40000) & (x_train_prep['total_rev_hi_lim'] <= 55000), 1, 0)\n",
    "x_train_prep['total_rev_hi_lim:55K_95K'] = np.where((x_train_prep['total_rev_hi_lim'] > 55000) & (x_train_prep['total_rev_hi_lim'] <= 95000), 1, 0)\n",
    "x_train_prep['total_rev_hi_lim:>95K'] = np.where((x_train_prep['total_rev_hi_lim'] > 95000), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_data == \"train\":\n",
    "    list_of_dummy_variables.extend([\"total_rev_hi_lim:<=5K\", \"total_rev_hi_lim:5K_10K\", \"total_rev_hi_lim:10K_20K\", \"total_rev_hi_lim:20K_30K\", \n",
    "                                    \"total_rev_hi_lim:30K_40K\", \"total_rev_hi_lim:40K_55K\", \"total_rev_hi_lim:55K_95K\", \"total_rev_hi_lim:>95K\"])\n",
    "    list_of_ref_categories.append(\"total_rev_hi_lim:<=5K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_ref_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([x_train_prep, y_train_prep], axis=1)\n",
    "dataset.to_csv(f\"./data/processed_{input_data}.csv\", index=False)\n",
    "\n",
    "\n",
    "if input_data == \"train\":\n",
    "    cat_varibels = pd.DataFrame({\"woe_vars\": list_of_dummy_variables})\n",
    "    ref_vars = pd.DataFrame({\"ref_vars\": list_of_ref_categories})\n",
    "    cat_varibels.to_csv(f\"./data/woe_cat_vars.csv\", index=False)\n",
    "    ref_vars.to_csv(f\"./data/woe_ref_vars.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, x_train, x_train_prep, y_train, y_train_prep, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whiskey/miniconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_348203/2274191625.py:1: DeprecationWarning: `import pandas_profiling` is going to be deprecated by April 1st. Please use `import ydata_profiling` instead.\n",
      "  from pandas_profiling import ProfileReport\n"
     ]
    }
   ],
   "source": [
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1310/1310 [05:39<00:00,  3.85it/s, Completed]                                                       \n",
      "Generate report structure: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.71s/it]\n",
      "Render HTML: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.38s/it]\n",
      "Export report to file: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13.61it/s]\n"
     ]
    }
   ],
   "source": [
    "profile.to_file(\"data_profile.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
